# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FVHvPhBoiPzsY65BVHcQBcivlR46l3Kt
"""

import pandas as pd
data_path = '/content/rating_complete.csv'
data = pd.read_csv(data_path)

# Assuming the uploaded file is named "example_dataset.csv"
df = pd.read_csv('rating_complete.csv')

# data shape
df.shape

df.isnull().sum()
df.isna().sum()

!pip install seaborn
import seaborn as sns

sns.countplot(x='rating',data=df)

import numpy as np
top_10=np.array(df['anime_id'].value_counts().sort_values(ascending=False).head(10).index)

filtered_rating=df[df['anime_id'].isin(top_10)]

filtered_rating

filtered_rating.shape

sns.countplot(x='rating',data=filtered_rating)

filtered_rating[filtered_rating['rating']>=9].count()

import pandas as pd
data_path = '/content/anime.csv'
data = pd.read_csv(data_path)

# Assuming the uploaded file is named "example_dataset.csv"
anime_df = pd.read_csv('anime.csv')

anime_df.head()

anime_names = anime_df[anime_df['MAL_ID'].isin(top_10)]
anime_names.head(10)

anime_dic=dict(zip(anime_names["MAL_ID"],anime_names["Name"]))
anime_dic

filtered_rating['title']=filtered_rating['anime_id'].apply(lambda x: anime_dic[x])

filtered_rating

# obtain rewards
filtered_rating['reward']=filtered_rating['rating'].apply(lambda x: 0 if x<10 else 1)

filtered_rating

column_titles = ['user_id','anime_id','title','rating','reward']

filtered_rating.reindex(columns=column_titles)

filtered_rating.head()

# calculating the most liked anime out of the top 10 most reviewed movies
groups=filtered_rating.groupby("title")

anime_title=[]
anime_liked_percentage=[]
for title, title_df in groups:
    anime_title.append(title)
    anime_liked_percentage.append((np.sum(title_df["reward"])/len(title_df))*100)

liked_per_dic=dict(zip(anime_title,anime_liked_percentage))

liked_per_dic

import matplotlib.pyplot as plt
#color codes
c= ['#5f9ed1', '#595959', '#ababab', '#ff800e', '#006ba4', '#cfcfcf', '#ffbc79', '#a2c8ec', '#898989', '#c85200']

f = plt.figure()
f.set_figwidth(30)
f.set_figheight(10)
plt.bar(anime_title,height=anime_liked_percentage,color=c)
plt.title("Liked Percentage of Top 10 Most Reviewd Anime")

# save filtered data
filtered_rating.to_csv('final_anime.csv')

# obtain liked percentage per anime
liked_per=pd.DataFrame.from_dict(liked_per_dic,orient="index")
liked_per.to_csv("liked_per.csv")

anime_id_name=pd.DataFrame.from_dict(anime_dic,orient="index")
anime_id_name.to_csv("anime_id_name.csv")

filtered_rating.head()

import pandas as pd
data_path = '/content/final_anime.csv'
data = pd.read_csv(data_path)
dataset = pd.read_csv("final_anime.csv")

print(dataset.head())
print(dataset.shape)

# mapping anime_id to anime title
anime_id_name=pd.read_csv("anime_id_name.csv")
anime_id_name.columns=["anime_id","anime_title"]

anime_id_name

anime_id=anime_id_name["anime_id"]
anime_title=anime_id_name["anime_title"]
anime_id_name=dict(zip(anime_id,anime_title))

# mapping anime title to anime liked percentage
liked_per=pd.read_csv("liked_per.csv")
liked_per.columns=["anime_title","liked_percentage"]

liked_per

anime_title=liked_per["anime_title"]
anime_liked_per=liked_per["liked_percentage"]
liked_per=dict(zip(anime_title,anime_liked_per))

# defining logged data- contains only sequences of anime id/rewards
# 2 columns- anime_id, reward

logged_data=dataset.drop(labels=["Unnamed: 0","user_id","rating","title"],axis=1,inplace=False)

dataset.head()

logged_data.head()

logged_data.shape

# dictionary mapping arm number to anime id
anime_arm_id=dict(zip([i for i in range(10)],anime_id))

anime_arm_id

def normalise(array):
    maximum=max(array)
    minimum=min(array)
    for i in range(len(array)):
        array[i]=(array[i]-minimum)/(maximum-minimum)
    return array

def epsilon_greedy(k,eps,valid_recommendations,anime_arm_id,logged_data):

    # k : number of arms/number of total anime to be recommended
    # eps : exploration fraction-> algorithm will explore with a probability of eps and exploit with a probability of (1-eps)
    # valid_recommendations : total number of recommendations to be simulated in the online evaluation
    # anime_arm_id : dictionary mapping arm number to anime id
    # logged_data : data log to simulate online evaluation through the offline replayer method

    if len(anime_arm_id)!=k:
        print("The length of list of anime id's passed does not match the number of arms entered \n")
        return

    q = np.zeros(k)                   # the array of the estimated expected values of each arm/anime
    recommendations = 0
    anime_recom_number = np.zeros(k)
    reward = 0
    a = 0
    total_reward = 0
    avg_reward = np.zeros(valid_recommendations)

    while(recommendations != valid_recommendations):

        # drawing sample of size k times more than logged_data with replacement to implement bootstrapping
        sample = logged_data.sample(frac = k, replace=True)

        for i in range(len(sample)):

            # select arm
            p = np.random.rand() # randomly generates a number between 0 and 1

            if eps == 0 and recommendations == 0: # eps value indicated exploitation but since the reccomendations made are also zero there is no knowledge to exploit-randomly select any arm
                a = np.random.choice(k) # anime is chosen

            elif p > eps: # case of exploration
                a = np.argmax(q) # recommends the anime with the highest estimated expected value at current moment of time

            else: # case of exploitation
                a = np.random.choice(k) # anime is chosen randomly

            # checking if this recommendation is valid

            if(sample.iloc[i][0]==anime_arm_id[a]): # valid point->consider this for evaluation

                # storing the corresponding reward
                reward = sample.iloc[i][1]

                # updating counts
                recommendations+=1
                anime_recom_number[a]+=1

                # updating the rewards
                total_reward+=reward
                avg_reward[recommendations-1] = total_reward/recommendations

                # updating estimated expected value of the recommended anime
                q[a]=((q[a]*(anime_recom_number[a]-1))+reward)/anime_recom_number[a]

                if(recommendations==valid_recommendations): # stops the evaluation if valid number of recommendations have been made
                    break


    avg_reward = normalise(avg_reward) #fits the average rewards between a scale of 0 and 1
    best = np.argmax(q)

    # returns the estimated expected values of all thr anime after all the recommendations
    # the average normalised reward per iteration at each iteration
    # the number of recommendations made for each anime
    # the best anime arm number

    return best, q, avg_reward, anime_recom_number

def ucb_1(k,valid_recommendations,anime_arm_id,logged_data):

    if len(anime_arm_id)!=k:
        print("The length of list of anime id's passed does not match the number of arms entered \n")
        return

    q = np.zeros(k)                   # the array of the estimated expected values of each arm/anime
    recommendations = 0
    anime_recom_number=np.zeros(k)
    reward=0
    a=0
    total_reward = 0
    avg_reward=np.zeros(valid_recommendations) # the average reward per recommendation till the current recommendation
    round = 0

    # first each anime has to be recommended once
    for a in range(k): # for each anime
        round+=1
        # filtering data of "anime a" from logged data
        filt=(logged_data["anime_id"]==anime_arm_id[a])
        a_data=logged_data[filt]

        # choosing a random row in the filtered data
        i=np.random.choice(len(a_data))
        reward=a_data.iloc[i][1] # storing the reward for the particular anime

        # updating counts
        recommendations+=1
        anime_recom_number[a]+=1

        # updating the rewards
        total_reward+=reward
        avg_reward[recommendations-1]=total_reward/recommendations

        # updating estimated expected value of the recommended anime
        q[a]=((q[a]*(anime_recom_number[a]-1))+reward)/anime_recom_number[a]


    # now each anime has been recommended once, continuing with the algorithm
    while(recommendations!=valid_recommendations):

        # drawing sample of size k times more than logged_data with replacement to implement bootstrapping
        sample=logged_data.sample(frac=k,replace=True)

        for i in range(len(sample)):
            round+=1
            # select arm
            upper_bound_arm = q + np.sqrt(np.divide(2*np.log10(round),anime_recom_number))
            a=np.argmax(upper_bound_arm) # choosing the anime with maximum upper bound

            # checking if this recommendation is valid
            if(sample.iloc[i][0]==anime_arm_id[a]):

                # storing the corresponding reward
                reward=sample.iloc[i][1]

                # updating counts
                recommendations+=1
                anime_recom_number[a]+=1

                # updating the rewards
                total_reward+=reward
                avg_reward[recommendations-1]=total_reward/recommendations

                # updating estimated expected value of the recommended anime
                q[a]=((q[a]*(anime_recom_number[a]-1))+reward)/anime_recom_number[a]

                if(recommendations==valid_recommendations): # stops the evaluation if valid number of recommendations have been made
                    break


    avg_reward=normalise(avg_reward) # fits the average rewards between a scale of 0 and 1
    best=np.argmax(q)

    return best,q,avg_reward,anime_recom_number

def thompson_sampling(k,valid_recommendations,anime_arm_id,logged_data):

    if len(anime_arm_id)!=k:
        print("The length of list of anime id's passed does not match the number of arms entered \n")
        return

    q = np.zeros(k)                   # the array of the estimated expected values of each arm/anime
    recommendations = 0
    anime_recom_number=np.zeros(k)
    reward=0
    a=0
    total_reward = 0
    avg_reward = np.zeros(valid_recommendations) # the average reward per recommendation till the current recommendation

    # a and b for each of anime has been initialised to 1
    # this will correspond to each arm having a uniform distribution initially
    a_item = np.ones(k)
    b_item = np.ones(k)


    while(recommendations!=valid_recommendations):

        # drawing sample of size k times more than logged_data with replacement to implement bootstrapping
        sample=logged_data.sample(frac=k,replace=True)

        for i in range(len(sample)):

            # select arm
            # choose the arm which maximises the value returned from the beta function
            beta_val_item=np.ones(k) # holds the value from the beta distribution for each item
            # sample a value from the beta distribution of all k arms
            for j in range(k):
                beta_val_item[j] = np.random.beta(a_item[j],b_item[j])
            # pull the arm whose sampled value is high
            a = np.argmax(beta_val_item)

            # checking if this recommendation is valid
            if(sample.iloc[i][0]==anime_arm_id[a]):

                # storing the corresponding reward
                reward = sample.iloc[i][1]

                # update alpha or beta value
                if(reward==1): # success
                    a_item[a]+=1
                else:          # failure
                    b_item[a]+=1

                # updating counts
                recommendations+=1
                anime_recom_number[a]+=1

                # updating rewards
                total_reward+=reward
                avg_reward[recommendations-1]=total_reward/recommendations

                # updating estimated expected value of the recommended item
                q[a]=((q[a]*(anime_recom_number[a]-1))+reward)/anime_recom_number[a]

                if (recommendations==valid_recommendations): # stops the evaluation if valid number of recommendations have been made
                    break


    avg_reward_normalize = normalise(avg_reward)
    best=np.argmax(q)

    return best, q, avg_reward_normalize, anime_recom_number

from tqdm import tqdm
k=10
eps=0.1
valid_recommendations=1000
episodes=1000 #each epsisode is an experiment of all algorithms with 1000 valid recommendations

#long term q
q_eps_long=np.zeros(k)
q_ucb_long=np.zeros(k)
q_ts_long=np.zeros(k)

#long term avg rewards
avg_reward_eps_long=np.zeros(valid_recommendations)
avg_reward_ucb_long=np.zeros(valid_recommendations)
avg_reward_ts_long=np.zeros(valid_recommendations)

#long term anime recom number
anime_recom_number_eps_long=np.zeros(k)
anime_recom_number_ucb_long=np.zeros(k)
anime_recom_number_ts_long=np.zeros(k)


#best arm->number of times each arm has been returned as the best arm
best_arm_eps=np.zeros(k)
best_arm_ucb=np.zeros(k)
best_arm_ts=np.zeros(k)


for i in tqdm(range(episodes)):

    best_eps,q_eps,avg_reward_eps,anime_recom_number_eps=epsilon_greedy(k,eps,valid_recommendations,anime_arm_id,logged_data)
    best_ucb,q_ucb,avg_reward_ucb,anime_recom_number_ucb=ucb_1(k,valid_recommendations,anime_arm_id,logged_data)
    best_ts,q_ts,avg_reward_ts,anime_recom_number_ts=thompson_sampling(k,valid_recommendations,anime_arm_id,logged_data)

    #long term estimated expected rewards
    q_eps_long=q_eps_long+(q_eps-q_eps_long)/(i+1)
    q_ucb_long=q_ucb_long+(q_ucb-q_ucb_long)/(i+1)
    q_ts_long=q_ts_long+(q_ts-q_ts_long)/(i+1)

    #long term avg rewards
    avg_reward_eps_long=avg_reward_eps_long+(avg_reward_eps-avg_reward_eps_long)/(i+1)
    avg_reward_ucb_long=avg_reward_ucb_long+(avg_reward_ucb-avg_reward_ucb_long)/(i+1)
    avg_reward_ts_long=avg_reward_ts_long+(avg_reward_ts-avg_reward_ts_long)/(i+1)


    #long term anime recom number
    anime_recom_number_eps_long=anime_recom_number_eps_long+(anime_recom_number_eps-anime_recom_number_eps_long)/(i+1)
    anime_recom_number_ucb_long=anime_recom_number_ucb_long+(anime_recom_number_ucb-anime_recom_number_ucb_long)/(i+1)
    anime_recom_number_ts_long=anime_recom_number_ts_long+(anime_recom_number_ts-anime_recom_number_ts_long)/(i+1)

    #best arm updates
    best_arm_eps[best_eps]+=1
    best_arm_ucb[best_ucb]+=1
    best_arm_ts[best_ts]+=1

from numpy import savetxt
#long term q
savetxt('q_eps_long.csv', q_eps_long, delimiter=',')
savetxt('q_ucb_long.csv', q_ucb_long, delimiter=',')
savetxt('q_ts_long.csv', q_ts_long, delimiter=',')

#long term avg rewards
savetxt('avg_reward_eps_long.csv', avg_reward_eps_long, delimiter=',')
savetxt('avg_reward_ucb_long.csv', avg_reward_ucb_long, delimiter=',')
savetxt('avg_reward_ts_long.csv', avg_reward_ts_long, delimiter=',')

#long term anime recom number
savetxt('anime_recom_number_eps_long.csv', anime_recom_number_eps_long, delimiter=',')
savetxt('anime_recom_number_ucb_long.csv', anime_recom_number_ucb_long, delimiter=',')
savetxt('anime_recom_number_ts_long.csv', anime_recom_number_ts_long, delimiter=',')

#best arm->number of times each arm has been returned as the best arm
savetxt('best_arm_eps.csv', best_arm_eps, delimiter=',')
savetxt('best_arm_ucb.csv', best_arm_ucb, delimiter=',')
savetxt('best_arm_ts.csv', best_arm_ts, delimiter=',')

true_values=[liked_per[anime_id_name[i]]/100 for i in anime_id_name.keys()]
true_values
# optimal index is 3

expected_values=np.vstack((true_values,q_eps_long,
                          q_ucb_long,q_ts_long,))
data=pd.DataFrame(expected_values)

data.columns=anime_id_name.values()
data.index=["True Expected Rewards based on liked percentage",
            "Estimated Expected Reward for epsilon=0.1",
            "Estimated Expected Reward for UCB1",
            "Estimated Expected Reward for Thompson Sampling"
]

data

# saving the result
data.to_csv("estimated_expected_rewards.csv")

plt.figure(figsize=(14,10))
plt.plot(true_values, label="True values",linewidth=3)
plt.plot(q_eps_long, label=" Greedy=0.01",linewidth=3)
plt.plot(q_ucb_long, label="UCB1",linewidth=3)
plt.plot(q_ts_long, label="Thompson Sampling",linewidth=3)

plt.xlabel("Animations",fontsize=11)
plt.ylabel("Expected Rewards",fontsize=11)
plt.title("Comparison of Algorithms on the Expected Rewards per Animations",fontsize=15)
plt.legend(fontsize=12)
plt.show()

plt.figure(figsize=(14,10))

plt.plot(avg_reward_eps_long, label=" Greedy=0.01",linewidth=3)
plt.plot(avg_reward_ucb_long, label="UCB1",linewidth=3)
plt.plot(avg_reward_ts_long, label="Thompson Sampling",linewidth=3)

plt.xlabel("Iterations",fontsize=11)
plt.ylabel("Average Normalised Reward",fontsize=11)
plt.title("Comparison of Algorithms on the basis of Average Normalised Rewards per Iteration",fontsize=15)
plt.legend(fontsize=12)
plt.show()

# calculating the percentage anime recpmmendations of the most rewarding anime
algo= [ " Greedy (0.01)",
        "UCB1",
        "Thompson Sampling"
]

optimal_index=3

# calculating percentage arm pulls for each algorithm
percent_eps=round((anime_recom_number_eps_long[optimal_index]/np.sum(anime_recom_number_eps_long))*100)
percent_ucb=round((anime_recom_number_ucb_long[optimal_index]/np.sum(anime_recom_number_ucb_long))*100)
percent_ts=round((anime_recom_number_ts_long[optimal_index]/np.sum(anime_recom_number_ts_long))*100)

percent=[percent_eps,percent_ucb,percent_ts]

# Creating a figure with some fig size
fig, ax = plt.subplots(figsize = (12,5))
ax.bar(algo,percent,color="#ffbc79",width=0.2)

# So here index will give you x pos and data+1 will provide a little gap in y axis.
for index, data_ in enumerate(percent):
    plt.text(x=index , y =data_+1 , s=f"{data_}" , fontdict=dict(fontsize=10))
plt.tight_layout()
plt.xlabel("Algorithms",fontsize=11)
plt.ylabel("Percentage of Optimal Anime Recommendation",fontsize=11)
plt.title("Comparison of Algorithms on the basis of Percentage Optimal Recommendation ",fontsize=15)

algo= [ " Greedy (0.01)",
        "UCB1",
        "Thompson Sampling"]

optimal_index=3

# calculating percentage correct choices for best arm
per_eps=round((best_arm_eps[optimal_index]/np.sum(best_arm_eps))*100)
per_ucb=round((best_arm_ucb[optimal_index]/np.sum(best_arm_ucb))*100)
per_ts=round((best_arm_ts[optimal_index]/np.sum(best_arm_ts))*100)

percent=[per_eps,per_ucb,per_ts]

# Creating a figure with some fig size
fig, ax = plt.subplots(figsize = (12,5))
ax.bar(algo,percent,color="#5f9ed1",width=0.2)

# So here index will give you x pos and data+1 will provide a little gap in y axis.
for index, data_ in enumerate(percent):
    plt.text(x=index , y =data_+1 , s=f"{data_}" , fontdict=dict(fontsize=10))
plt.tight_layout()
plt.xlabel("Algorithms",fontsize=12)
plt.ylabel("Percentage of Correct Optimal Choice",fontsize=12)
plt.title("Comparison of Algorithms on the basis of Percentage of Correct Optimal ",fontsize=15)
plt.show()

"""Cross-validation and Testing"""

from sklearn.model_selection import KFold

# Reduce the number of splits
kf = KFold(n_splits=4, shuffle=True, random_state=42)  # Assuming you have at least 4 samples
for train_index, test_index in kf.split(data):
    train_data, test_data = data.iloc[train_index], data.iloc[test_index]
    # Fit and test your model here

from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
for train_index, test_index in loo.split(data):
    train_data, test_data = data.iloc[train_index], data.iloc[test_index]
    # Fit and test your model here

print(data.shape)  # Check the number of samples and features
print(data.head())  # Preview the first few rows of the dataset

import numpy as np

# Data for each algorithm's estimated expected rewards
true_rewards = np.array([0.326134, 0.340324, 0.219229, 0.526486, 0.220682, 0.158211, 0.260251, 0.210291, 0.127995, 0.241703])
epsilon_rewards = np.array([0.312985, 0.291680, 0.186989, 0.487199, 0.194709, 0.147094, 0.232331, 0.187010, 0.117335, 0.210738])
ucb_rewards = np.array([0.308222, 0.323700, 0.201629, 0.523687, 0.204009, 0.143438, 0.245882, 0.196969, 0.115931, 0.223125])
ts_rewards = np.array([0.278855, 0.291521, 0.175670, 0.525696, 0.180623, 0.125154, 0.214011, 0.171861, 0.096694, 0.201052])

# Find the index of the best arm based on true expected rewards
best_arm_index = np.argmax(true_rewards)

# Calculate TP and FP for each algorithm
def calculate_f1_scores(estimated_rewards, best_arm_index):
    tp = (estimated_rewards.argmax() == best_arm_index).sum()
    fp = (estimated_rewards.argmax() != best_arm_index).sum()
    precision = tp / (tp + fp)
    recall = tp / tp  # Recall is 1 as we are not defining FN
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score

# Calculate F1 scores for each algorithm
f1_eps = calculate_f1_scores(epsilon_rewards, best_arm_index)
f1_ucb = calculate_f1_scores(ucb_rewards, best_arm_index)
f1_ts = calculate_f1_scores(ts_rewards, best_arm_index)

print(f"F1 Score for Epsilon-Greedy: {f1_eps}")
print(f"F1 Score for UCB1: {f1_ucb}")
print(f"F1 Score for Thompson Sampling: {f1_ts}")

import numpy as np     #P-value
from scipy.stats import ttest_1samp

# Assuming `data` is your DataFrame
true_rewards = data.loc['True Expected Rewards based on liked percentage']
epsilon_rewards = data.loc['Estimated Expected Reward for epsilon=0.1']
ucb_rewards = data.loc['Estimated Expected Reward for UCB1']
ts_rewards = data.loc['Estimated Expected Reward for Thompson Sampling']

# Perform one-sample t-tests against the true expected rewards
t_stat_eps, p_val_eps = ttest_1samp(epsilon_rewards - true_rewards, 0)
t_stat_ucb, p_val_ucb = ttest_1samp(ucb_rewards - true_rewards, 0)
t_stat_ts, p_val_ts = ttest_1samp(ts_rewards - true_rewards, 0)

print(f"Epsilon-Greedy P-Value: {p_val_eps}")
print(f"UCB1 P-Value: {p_val_ucb}")
print(f"Thompson Sampling P-Value: {p_val_ts}")

#Interpretation of T-Test Results
import numpy as np

# Simulate algorithm performance data
# For example, let's assume these are rewards collected over 100 trials
algorithm1_performance = np.random.normal(loc=50, scale=10, size=100)  # Normal distribution centered at 50
algorithm2_performance = np.random.normal(loc=45, scale=10, size=100)  # Normal distribution centered at 45

from scipy import stats

# Perform a t-test between two algorithms
t_stat, p_value = stats.ttest_ind(algorithm1_performance, algorithm2_performance)
print(f"T-statistic: {t_stat}, P-value: {p_value}")

#Interpretation of Beta Values from Thompson Sampling
import numpy as np

# Number of arms
k = 5

# Initialize the count of successes and failures for each arm
# 'a_item' represents the count of successes + baseline success factor
# 'b_item' represents the count of failures + baseline failure factor
a_item = np.ones(k)  # Adding a baseline success factor of 1
b_item = np.ones(k)  # Adding a baseline failure factor of 1

# Assume 'context' is a vector with contextual information affecting success probability
context = np.random.rand(k)  # Random context for demonstration; replace with actual contextual data

# Placeholder for storing beta distribution values
beta_val_item = np.zeros(k)

# Example loop to calculate beta distribution values for each arm
for j in range(k):
    beta_val_item[j] = np.random.beta(a_item[j] + context[j], b_item[j])

# Display the generated beta values
print("Beta values for each arm:", beta_val_item)

"""Monitoring and Adaptation"""

import numpy as np

# Define the function to select an arm based on beta values
def select_arm(beta_values):
    """
    Select an arm based on the highest beta value (highest estimated probability of reward).

    Args:
    beta_values (np.array): Array containing the beta values for each arm.

    Returns:
    int: The index of the selected arm.
    """
    return np.argmax(beta_values)  # Select the arm with the highest beta value

# Define the function to simulate interaction with the selected arm
def simulate_interaction(arm_index):
    """
    Simulate an interaction with the selected arm. This is a placeholder function and should be tailored
    to reflect how an interaction with an arm yields a reward in your specific application.

    Args:
    arm_index (int): The index of the selected arm.

    Returns:
    int: The reward from the interaction, 0 for no reward, 1 for reward.
    """
    # Example: Simulate based on some probability of success for simplicity
    # In practice, this should be based on your application's dynamics
    success_probability = 0.1 + 0.05 * arm_index  # Just a simple increasing probability with arm index
    return np.random.binomial(1, success_probability)

# Assume initial beta_values and a_item, b_item setup
beta_values = np.random.rand(5)  # Randomly generated beta values for 5 arms
a_item = np.ones(5)  # Success counts initialized to 1 for each arm
b_item = np.ones(5)  # Failure counts initialized to 1 for each arm

# Function to update bandit parameters based on the outcome of an interaction
def update_bandit_parameters(arm_index, reward, a_item, b_item):
    if reward > 0:
        a_item[arm_index] += 1  # Increment success parameter
    else:
        b_item[arm_index] += 1  # Increment failure parameter

# Simulate an interaction
arm_index = select_arm(beta_values)  # Select an arm based on current beta values
reward = simulate_interaction(arm_index)  # Simulate the interaction for the selected arm
update_bandit_parameters(arm_index, reward, a_item, b_item)

print("Updated success counts (a_item):", a_item)
print("Updated failure counts (b_item):", b_item)

import numpy as np

# Example definitions for select_arm and simulate_interaction functions
def select_arm(beta_values):
    """Select an arm based on the highest beta value."""
    return np.argmax(beta_values)  # Select the arm with the highest beta value

def simulate_interaction(arm_index):
    """Simulate an interaction with the selected arm.
       Let's assume a simple probability of success that increases with the arm index."""
    success_probability = 0.1 + 0.1 * arm_index  # Simple probability model for example
    return np.random.binomial(1, success_probability)  # Simulate success/failure

def update_bandit_parameters(arm_index, reward, a_item, b_item):
    """Update bandit parameters based on the reward received from the interaction."""
    if reward > 0:
        a_item[arm_index] += 1  # Increment success parameter
    else:
        b_item[arm_index] += 1  # Increment failure parameter

# Initializing beta values for demonstration
beta_values = np.random.rand(5)  # Random beta values for 5 arms

# Initializing arrays for success and failure counts
a_item = np.ones(5)  # Success counts
b_item = np.ones(5)  # Failure counts

# Perform multiple simulations to see varying results
for _ in range(10):  # Run this block multiple times to simulate multiple interactions
    arm_index = select_arm(beta_values)
    reward = simulate_interaction(arm_index)
    update_bandit_parameters(arm_index, reward, a_item, b_item)

    # Output the selected arm, reward, and updated parameters
    print(f"Selected Arm: {arm_index}, Reward: {reward}")
    print(f"Updated a_item: {a_item}, Updated b_item: {b_item}")

    #Outputs Information: After each interaction, it prints:
    #The index of the selected arm.
    #The reward received (0 or 1 in this simulation).
    #The updated counts of successes and failures for each arm (a_item and b_item).

"""Assessing Practical Significance"""

import numpy as np

# Simulating some example data for demonstration purposes
# Let's assume you have data for 12 months

# Example monthly_rewards: List of arrays where each array represents the rewards collected each day in a month
monthly_rewards = [np.random.randint(0, 10, size=30) for _ in range(12)]  # Random daily rewards for 12 months i.e., direct feedback

# Example monthly_sales: Array or list of sales figures corresponding to each month
monthly_sales = np.random.randint(100, 200, size=12)  # Random sales figures for 12 months i.e.,revenue or number of items sold per month as a result of the recommendations.

# Function to assess the impact of rewards on business metrics
def assess_impact_on_metrics(reward_history, business_metrics):
    # Analyze trends in the rewards and how they correlate with business outcomes
    correlation = np.corrcoef(reward_history, business_metrics)
    print(f"Correlation between bandit rewards and business metrics: {correlation[0, 1]}")

# Calculate the sum of rewards for each month to create a reward history
reward_history = [np.sum(rewards) for rewards in monthly_rewards]  # Monthly sum of rewards

# Now, use the defined function to assess the impact
assess_impact_on_metrics(reward_history, monthly_sales)

from sklearn.metrics import precision_score, recall_score, f1_score

# Example data
# true_labels are the actual interactions (1 if positive, 0 if negative)
# predictions are what the algorithm predicted
true_labels = [1, 0, 1, 1, 0, 1, 0, 1, 0, 1]
predictions = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]

precision = precision_score(true_labels, predictions)
recall = recall_score(true_labels, predictions)
f1 = f1_score(true_labels, predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

!pip install --upgrade nbconvert
!jupyter nbconvert --to html /content/Untitled14.ipynb